{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "N-BEATS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPyqIwg2QmjLklQzBUZ/i0Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maxmatical/gld_forecasting/blob/master/N_BEATS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1wjsEdqEFyN",
        "colab_type": "text"
      },
      "source": [
        "Article: https://towardsdatascience.com/n-beats-beating-statistical-models-with-neural-nets-28a4ba4a4de8\n",
        "\n",
        "Pytorch: https://github.com/philipperemy/n-beats\n",
        "\n",
        "Kaggle/Fastai: https://www.kaggle.com/neoyipeng2018/forecasting-btc-using-n-beats/ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ru0C1WLxD1Wg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from fastai.tabular import *\n",
        "from fastai.callbacks import *\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMKtAJ6TP90W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "act_fn = nn.ReLU(inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iC1re9toEAiS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class block(nn.Module):\n",
        "    def __init__(self, ni, nh, theta_dim, n_out, bn:bool = True, ps:float=0., actn=None):\n",
        "        \"\"\"\n",
        "        ni = backcast length\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        layers = [*bn_drop_lin(ni,nh,bn,ps,actn),\n",
        "                  *bn_drop_lin(nh,nh,bn,ps,actn),\n",
        "                  *bn_drop_lin(nh,nh,bn,ps,actn),\n",
        "                  *bn_drop_lin(nh,nh,bn,ps,actn)]\n",
        "        self.ff_block = nn.Sequential(*layers)\n",
        "        self.fwd = nn.Sequential(*bn_drop_lin(nh,theta_dim,bn,ps,actn),\n",
        "                                 *bn_drop_lin(theta_dim,n_out,bn,ps, actn = None)) # no act fn on fwd and bwd forecast\n",
        "        self.bwd = nn.Sequential(*bn_drop_lin(nh,theta_dim,bn,ps,actn),\n",
        "                                 *bn_drop_lin(theta_dim,ni,bn,ps, actn = None))\n",
        "\n",
        "    def forward(sef, x):\n",
        "        x = self.ff_block(x)\n",
        "        x_fwd = self.fwd(x)\n",
        "        x_bwd = self.bwd(x)\n",
        "\n",
        "        return(x-x_bwd, x_fwd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFCaK2RBQjPr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class nbeats(Module):\n",
        "    def __init__(self, nh, theta_dim, emb_szs:ListSizes, n_blocks n_cont:int, out_sz:int, layers:Collection[int], ps:Collection[float]=None,\n",
        "                 emb_drop:float=0., y_range:OptRange=None, use_bn:bool=True, bn_final:bool=False):\n",
        "        super().__init__()\n",
        "        self.embeds = nn.ModuleList([embedding(ni, nf) for ni,nf in emb_szs])\n",
        "        self.emb_drop = nn.Dropout(emb_drop)\n",
        "        self.bn_cont = nn.BatchNorm1d(n_cont)\n",
        "        n_emb = sum(e.embedding_dim for e in self.embeds)\n",
        "        self.n_emb,self.n_cont,self.y_range = n_emb,n_cont,y_range\n",
        "        self.ni  = self.n_emb + self.n_cont\n",
        "        self.n_out = out_sz\n",
        "        self.nh = nh\n",
        "        self.theta_dim = theta_dim\n",
        "        self.n_blocks = n_blocks\n",
        "\n",
        "        self.stack = nn.ModuleList()\n",
        "        for i in range(self.n_blocks):\n",
        "            self.stack.append(block(self.ni, self.nh, self.theta_dim, self.n_out, ps, act_fn))\n",
        "\n",
        "\n",
        "        def forward(self, x_cat, x_cont):\n",
        "            out = 0 \n",
        "            if self.n_emb != 0:\n",
        "                x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n",
        "                x = torch.cat(x, 1)\n",
        "                x = self.emb_drop(x)\n",
        "            if self.n_cont != 0:\n",
        "                x_cont = self.bn_cont(x_cont)\n",
        "                x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont\n",
        "\n",
        "            for i, b in enumerate(self.stack):\n",
        "                x, x_fwd = b(x)\n",
        "                out += x_fwd\n",
        "\n",
        "            if self.y_range is not None: # squeezing to y_range\n",
        "                out = (self.y_range[1]-self.y_range[0]) * torch.sigmoid(out) + self.y_range[0]\n",
        "\n",
        "            return out\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kly5MJxRcm-A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forecast_learner(data:DataBunch, n_hidden, theta_dim, layers:Collection[int], emb_szs:Dict[str,int]=None, metrics=None,\n",
        "        ps:Collection[float]=None, emb_drop:float=0., y_range:OptRange=None, use_bn:bool=True, n_blocks = 6, **learn_kwargs):\n",
        "    \"Get a `Learner` using `data`, with `metrics`, including a `TabularModel` created using the remaining params.\"\n",
        "    emb_szs = data.get_emb_szs(ifnone(emb_szs, {}))\n",
        "    model = nbeats(n_hidden, theta_dim, n_blocks, emb_szs, len(data.cont_names), out_sz=data.c, layers=layers, ps=ps, emb_drop=emb_drop,\n",
        "                    y_range=y_range, use_bn=use_bn)\n",
        "    return Learner(data, model, metrics=metrics, **learn_kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hc7F-xs5cnjG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}